{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=gpu1\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: GeForce GTX 690 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.utils import floatX\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from utils import build_model, iterate_minibatches\n",
    "from data_prep import b01c_to_bc01, data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from visualize import plot_loss, plot_conv_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=0\n",
    "num_classes=23\n",
    "toy=1\n",
    "caffe_ref_path = '../models/theano_caffe_ref/caffe_reference.pkl'\n",
    "X_path='../datasets/msrcv2/Xaug_b01c.npy'\n",
    "Y_path='../datasets/msrcv2/Y.npy'\n",
    "MEAN_IMG_PATH='../models/theano_caffe_ref/ilsvrc_2012_mean.npy'\n",
    "batchsize=96\n",
    "num_epochs=1000\n",
    "snapshot=200 # save model after 200 epochs\n",
    "p=0.5 # drop out prob.\n",
    "lambda2=0.0005/2 # l2-regularizer constant\n",
    "\n",
    "if toy:\n",
    "    num_epochs=30\n",
    "    snapshot=10\n",
    "    batchsize=10\n",
    "    #p=0\n",
    "    #lambda2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### LOADING DATA\n",
    "X = np.load(X_path)\n",
    "Y = np.load(Y_path)\n",
    "MEAN_IMG = np.load(MEAN_IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split train/val/test set\n",
    "indicies = np.arange(len(Y))\n",
    "Y_train_val, Y_test, idx_train_val, idx_test = train_test_split(Y, indicies, \n",
    "    random_state=seed, train_size=float(2)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train, Y_val, idx_train, idx_val = train_test_split(Y_train_val, idx_train_val, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/val/test set size: 295 99 197\n"
     ]
    }
   ],
   "source": [
    "print \"Train/val/test set size:\",len(idx_train),len(idx_val),len(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO flips\n",
    "\n",
    "idx_aug_train = data_aug(idx_train, mode='aug', isMat='idx')\n",
    "Xaug_train = b01c_to_bc01(X[idx_aug_train])\n",
    "Yaug_train = data_aug(Y_train, mode='aug', isMat='Y')\n",
    "\n",
    "idx_aug_val = data_aug(idx_val, mode='aug', isMat='idx')\n",
    "Xaug_val = b01c_to_bc01(X[idx_aug_val])\n",
    "Yaug_val = data_aug(Y_val, mode='aug', isMat='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented train/val set size: 1475 495\n"
     ]
    }
   ],
   "source": [
    "print \"Augmented train/val set size:\",len(Xaug_train),len(Yaug_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if toy: # try to overfit a tiny subset of the data\n",
    "    Xaug_train = Xaug_train[:batchsize*2]\n",
    "    Yaug_train = Yaug_train[:batchsize*2]\n",
    "    Xaug_val = Xaug_val[:batchsize]\n",
    "    Yaug_val = Yaug_val[:batchsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.imatrix('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = build_model(caffe_ref_path, num_classes, input_var, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for training\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.binary_crossentropy(prediction, target_var) \n",
    "\n",
    "weights = lasagne.layers.get_all_params(network, regularizable=True)\n",
    "loss = loss.mean() + theano.shared(floatX(lambda2))*T.sum([T.sum(w ** 2) for w in weights])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = theano.shared(np.array(0.01, dtype=theano.config.floatX))\n",
    "lr_decay = np.array(0.3, dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create update expressions for training\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=lr, momentum=0.9)\n",
    "# ! TODO adjust for per-layer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for validation/testing\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.binary_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,)\n",
    "\n",
    "# Compile a second function computing the validation loss:\n",
    "val_fn = theano.function([input_var, target_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_history={}\n",
    "training_history['training_loss'] = []\n",
    "training_history['validation_loss'] = []\n",
    "training_history['learning_rate'] = []\n",
    "training_history['epoch_time'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch\tTrain Loss\tValid Loss\tTime\tLearning rate\n",
      "1\t1.654835\t0.462565\t0.537s\t0.00999999977648\n",
      "2\t1.098222\t0.322888\t0.492s\t0.00999999977648\n",
      "3\t0.993433\t0.357139\t0.480s\t0.00999999977648\n",
      "4\t0.959984\t0.366446\t0.474s\t0.00999999977648\n",
      "5\t0.895165\t0.334674\t0.496s\t0.00999999977648\n",
      "6\t0.839535\t0.330638\t0.490s\t0.00999999977648\n",
      "7\t0.794487\t0.317508\t0.506s\t0.00999999977648\n",
      "8\t0.753340\t0.295795\t0.499s\t0.00999999977648\n",
      "9\t0.727279\t0.301817\t0.490s\t0.00999999977648\n",
      "10\t0.708752\t0.313290\t0.491s\t0.00999999977648\n",
      "11\t0.693198\t0.318950\t0.487s\t0.00300000002608\n",
      "12\t0.679026\t0.326207\t0.480s\t0.00300000002608\n",
      "13\t0.674395\t0.339983\t0.493s\t0.00300000002608\n",
      "14\t0.674608\t0.340451\t0.488s\t0.00300000002608\n",
      "15\t0.660730\t0.343588\t0.473s\t0.00300000002608\n",
      "16\t0.669289\t0.337010\t0.473s\t0.00300000002608\n",
      "17\t0.663192\t0.340105\t0.483s\t0.00300000002608\n",
      "18\t0.665150\t0.345344\t0.475s\t0.00300000002608\n",
      "19\t0.658888\t0.356052\t0.474s\t0.00300000002608\n",
      "20\t0.658248\t0.367638\t0.472s\t0.00300000002608\n",
      "21\t0.657248\t0.377309\t0.476s\t0.000900000042748\n",
      "22\t0.654110\t0.385612\t0.475s\t0.000900000042748\n",
      "23\t0.654316\t0.390886\t0.482s\t0.000900000042748\n",
      "24\t0.655133\t0.395073\t0.480s\t0.000900000042748\n",
      "25\t0.650269\t0.397705\t0.474s\t0.000900000042748\n",
      "26\t0.651870\t0.399351\t0.480s\t0.000900000042748\n",
      "27\t0.653211\t0.399106\t0.471s\t0.000900000042748\n",
      "28\t0.652538\t0.398656\t0.474s\t0.000900000042748\n",
      "29\t0.655796\t0.397937\t0.491s\t0.000900000042748\n",
      "30\t0.654833\t0.396192\t0.478s\t0.000900000042748\n"
     ]
    }
   ],
   "source": [
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "print(\"\\nEpoch\\tTrain Loss\\tValid Loss\\tTime\\tLearning rate\")\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):    \n",
    "        # TODO - Early stopping http://deeplearning.net/tutorial/gettingstarted.html#early-stopping\n",
    "\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(Xaug_train, Yaug_train, batchsize, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:       \n",
    "        val_err = 0\n",
    "        #val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(Xaug_val, Yaug_val, batchsize, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            #val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Record training history\n",
    "        training_history['epoch_time'].append(time.time() - start_time)\n",
    "        training_history['training_loss'].append(train_err / train_batches)\n",
    "        training_history['validation_loss'].append(val_err / val_batches)\n",
    "        training_history['learning_rate'].append(lr.get_value())\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"{}\\t{:.6f}\\t{:.6f}\\t{:.3f}s\\t{}\".format(\n",
    "                epoch + 1, \n",
    "                training_history['training_loss'][-1],\n",
    "                training_history['validation_loss'][-1],\n",
    "                training_history['epoch_time'][-1],\n",
    "                training_history['learning_rate'][-1]\n",
    "            ))\n",
    "            #print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "             #   val_acc / val_batches * 100))    \n",
    "\n",
    "        if (epoch+1)%10==0: # ! TODO Condition for learning rate decay\n",
    "            lr.set_value(lr.get_value() * lr_decay) \n",
    "            \n",
    "        if (epoch+1)%snapshot==0:\n",
    "            time_stamp=time.strftime(\"%y%m%d%H%M%S\", time.localtime())           \n",
    "            snapshot_path_string = '../snapshot_models/'+str(num_classes)+'alex'+time_stamp+'_'+str(epoch+1)\n",
    "            np.savez(snapshot_path_string+'.npz', lasagne.layers.get_all_param_values(network))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "# Save model after num_epochs or KeyboardInterrupt\n",
    "if (epoch+1)%snapshot!=0: # to avoid duplicate save\n",
    "    time_stamp=time.strftime(\"%y%m%d%H%M%S\", time.localtime())\n",
    "    snapshot_path_string = '../snapshot_models/'+str(num_classes)+'alex'+time_stamp+'_'+str(epoch+1)\n",
    "    np.savez(snapshot_path_string+'.npz', lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And load them again later on like this:\n",
    "#with np.load('../snapshot_models/23alex16042023213910.npz') as f:\n",
    "#    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "# lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_loss(training_history, snapshot_path_string+'_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_conv_weights(lasagne.layers.get_all_layers(network)[1], snapshot_path_string+'_conv1weights_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
