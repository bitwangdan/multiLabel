{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "exclude label \"horses\" and \"mountains\" for the numbers are small, following the publisher suggestion [MSRCv2]\n",
    "\n",
    "## Preprocess\n",
    "* Resize to 256x256 (due to model requirements), following [AlexNet] procedure:\n",
    "    * resize square images to 256x256. If an image is rectangular, let the shorter edge be 256px and **crop** the\n",
    "    central patch of the image\n",
    "        * *Cropping might eliminate valuable information*\n",
    "    * substract pixel intensities on each R, G, B channel by their respective channel's mean intensity\n",
    "        * *should we scale to [-1,1] interval?*\n",
    "\n",
    "* Store data in Python pickle format. It make data transfer to GPU much faster [[?]](#Ref).\n",
    "\n",
    "Resized raw datasize: 256 x 256 x 3 x 591 $\\approx$ 110MB. So we have ~500 examples of 65536 dimensions, which is a clear indication of possible over-fitting with deep nets.\n",
    "\n",
    "\n",
    "## Augmentation\n",
    "When training, we also perform data augmentation to avoid over-fitting. There are 2 forms of augmentation:\n",
    "* label-preserving transformation [e.g. AlexNet, 25, 4, 5](#Ref)\n",
    "    * translation: randomly generate patches of 224x224 on original images\n",
    "    * horizontal flip: to both original and generated images\n",
    "* altering intensities of the RGB channels [AlexNet](#Ref)\n",
    "    * *\"This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination.\"* [AlexNet](#Ref)\n",
    "    \n",
    "Prospective augmented datasize: order of $\\#$ random patches x $\\#$ splitting ratio $\\approx$ 2000 x 0.7 i.e. ~830,000 images. Number of random patches taken from [AlexNet](#Ref) as a rough guideline. I might consider generating only 200 random patches, due to hardware limitation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment Measures\n",
    "* Bipartion measure: on classification results (recall, precision, F1)\n",
    "    * Micro-averaged\n",
    "    * Macro-averaged\n",
    "* Ranking measure [[1]](#Ref): on ranked list of labels. One of either\n",
    "    * Rank loss: Section 3.1 of [1]\n",
    "    * One-Error: evaluates whether the top most ranked label with the highest score is is a positive label or not\n",
    "    * Coverage: measures on average how far one needs to go down the ranked list of labels to achieve recall of 100%\n",
    "    * Average Precision or AP measures the average fraction of labels preceding relevant labels in the ranked list of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "**Output (y) and target (t) format**\n",
    "* t = [0 0 1 1 ... ] in $Z^{k}$\n",
    "* y $\\in Z^{k}$ where elements are also binary predictors, thresholded on softmax probabilities of the k classes [1]\n",
    "\n",
    "**Cost function** \n",
    "\n",
    "Cross-entropy: CE is shown to be better in textual data and is faster to learn [[1]](#Ref). It's worth to note that similar statement is not found for image data yet, and **rank-loss** is also a common choice of cost function.\n",
    "\n",
    "<img src=\"http://ufldl.stanford.edu/wiki/images/math/7/6/3/7634eb3b08dc003aa4591a95824d4fbd.png\">[[2]](http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression#Cost_Function)\n",
    "* k: # classes\n",
    "* m: # examples\n",
    "\n",
    "Also need ***L2-regulariser***\n",
    "## Model Sketch \n",
    "* Architecture: as a starting point, we will consider architecture of existing models which worked on datasets of similar size (CIFAR?)\n",
    "    * Baseline model: 1 input, 2 hidden, 1 output (softmax) layer\n",
    "        * 800 neurons / hidden layer\n",
    "        * 0 or 20% drop-out at input layer, 50% drop-out on the hidden layers\n",
    "        * implemented with `nolearn` and `Lagsane`: out-of-the-box, not optimised for performance\n",
    "    * ConvNet model: 1 input, (?) Conv, (?) pooling, 1 output (softmax) layer\n",
    "        * allows deeper architecture with efficient learning time\n",
    "        * number of neurons and layers: **to be experimented**\n",
    "        * (most likely) 20% drop-out at input layer, 50% drop-out on the hidden layers\n",
    "        * implemented with `pylearn2`: optimised for performance\n",
    "* Threshold label predictor on softmax layer [[1]](#Ref)\n",
    "* ReLU units (neurons) on all layers, can be learned faster than sigmoid or tanh non-linearity [[?]](#Ref)\n",
    "* AdaGrad method for adaptive learning rate [[?]](#Ref)\n",
    "\n",
    "Dropout [[?]](#Ref) + data augmentation for regularisation as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning \n",
    " fine-tuning hyper-parameters and/or network architecture ***(ARGHH!!)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "compare to state-of-the-art performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension\n",
    "* consider rescale images to 256x256 regardless of aspect ratio. This assumes that object aspect ratios have litte effect to labeling.\n",
    "* experiment on other datasets (MIML, Corel 5k/10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref\n",
    "[1] Nam2014\n",
    "\n",
    "[AlexNet] Convolutional Neural Net model that broke state-of-the-art records, with significant performance, on ImageNet's image recognition (1 label per image) competition in 2012. \n",
    "\n",
    "[MSRCv2] MSRCv2 dataset\n",
    "\n",
    "[?] refs to be added later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
