{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasagne.updates.nesterov_momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "\n",
    "root_dir = \"/home/hoa/Desktop/multiLabel/\"\n",
    "os.chdir(root_dir+'pretrained/parameters_releasing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0_1 (128, 48, 5, 5)\n",
      "W0_3 (192, 192, 3, 3)\n",
      "W0_4 (128, 192, 3, 3)\n",
      "W1_1 (128, 48, 5, 5)\n",
      "W1_3 (192, 192, 3, 3)\n",
      "W1_4 (128, 192, 3, 3)\n",
      "W_0 (96, 3, 11, 11)\n",
      "W_2 (384, 256, 3, 3)\n",
      "W_5 (9216, 4096)\n",
      "W_6 (4096, 4096)\n",
      "W_7 (4096, 1000)\n",
      "b0_1 (128,)\n",
      "b0_3 (192,)\n",
      "b0_4 (128,)\n",
      "b1_1 (128,)\n",
      "b1_3 (192,)\n",
      "b1_4 (128,)\n",
      "b_0 (96,)\n",
      "b_2 (384,)\n",
      "b_5 (4096,)\n",
      "b_6 (4096,)\n",
      "b_7 (1000,)\n"
     ]
    }
   ],
   "source": [
    "Wnames = natsorted([w[:-4] for _,w in enumerate(glob.iglob('W*'))])\n",
    "\n",
    "Ws = {}\n",
    "for W in Wnames:\n",
    "    Ws[W] = np.load(W+\".npy\")\n",
    "    print W, Ws[W].shape\n",
    "    \n",
    "varName = natsorted([b[:-7] for _,b in enumerate(glob.iglob('b*'))])\n",
    "\n",
    "bs = {}\n",
    "for b in varName:\n",
    "    bs[b] = np.load(b+\"_65.npy\")\n",
    "    print b, bs[b].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\"\"\"Convert params to from c01b to bc01\"\"\"\n",
    "def dimConv(W):\n",
    "    assert len(W.shape)==4, 'not 4D tensor'\n",
    "    c,row,col,b = W.shape\n",
    "    out = np.ndarray((b,c,row,col), dtype=W.dtype)\n",
    "    for i in xrange(b):\n",
    "        out[i,:,:,:]=W[:,:,:,i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "for key in natsorted(Ws.keys()):\n",
    "    if len(Ws[key].shape)==4:\n",
    "        Ws[key]=dimConv(Ws[key])\n",
    "    print key, Ws[key].shape\n",
    "    np.save(key+\".npy\", Ws[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0_4 float32\n",
      "W0_1 float32\n",
      "W0_3 float32\n",
      "W_5 float32\n",
      "W1_4 float32\n",
      "W_7 float32\n",
      "W_6 float32\n",
      "W1_1 float32\n",
      "W_0 float32\n",
      "W1_3 float32\n",
      "W_2 float32\n"
     ]
    }
   ],
   "source": [
    "for key in Ws.keys():\n",
    "    print key, Ws[key].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the objects\n",
    "with open(\"Data/MSRCv2/lables.pkl\",'w') as f:\n",
    "    pickle.dump([labelNames, labelIDs], f)\n",
    "\n",
    "# Getting back the object\n",
    "with open(\"Data/MSRCv2/lables.pkl\",'r') as f:\n",
    "    test1, test2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you have a lot of data, you can reduce the file size by passing `protocol=-1` to `dump()`; `pickle` will then use the best available protocol instead of the default historical (and more backward-compatible) protocol. In this case, the file must be opened in binary mode (`wb` and `rb`, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeroplane' 'bicycle' 'bird' 'boat' 'body' 'book' 'building' 'car' 'cat'\n",
      " 'chair' 'cow' 'dog' 'face' 'flower' 'grass' 'mountain' 'road' 'sheep'\n",
      " 'sign' 'sky' 'tree' 'water']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aeroplane': 0,\n",
       " 'bicycle': 1,\n",
       " 'bird': 2,\n",
       " 'boat': 3,\n",
       " 'body': 4,\n",
       " 'book': 5,\n",
       " 'building': 6,\n",
       " 'car': 7,\n",
       " 'cat': 8,\n",
       " 'chair': 9,\n",
       " 'cow': 10,\n",
       " 'dog': 11,\n",
       " 'face': 12,\n",
       " 'flower': 13,\n",
       " 'grass': 14,\n",
       " 'mountain': 15,\n",
       " 'road': 16,\n",
       " 'sheep': 17,\n",
       " 'sign': 18,\n",
       " 'sky': 19,\n",
       " 'tree': 20,\n",
       " 'water': 21}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print test1; test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"Data/MSRCv2/Xtr.npy\", Xtr)\n",
    "np.save(\"Data/MSRCv2/Xva.npy\", Xva)\n",
    "np.save(\"Data/MSRCv2/Ytr.npy\", Ytr)\n",
    "np.save(\"Data/MSRCv2/Yva.npy\", Yva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr (441, 256, 256, 3)\n",
      "Xva (147, 256, 256, 3)\n",
      "Ytr (441, 22)\n",
      "Yva (147, 22)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path=\"Data/MSRCv2/\"):\n",
    "    dataset={}\n",
    "    setNames = [name[-7:-4] for _,name in enumerate(glob.iglob(path+\"*.npy\"))]\n",
    "    for name in setNames:\n",
    "        dataset[name]=np.load(path+name+\".npy\")\n",
    "        print name, dataset[name].shape\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def data_aug(mat, isinput=True, n_ops=10):\n",
    "    # resize, crop, mirror if \"input\", simply repeat if \"target\"\n",
    "    n_samples = mat.shape[0]\n",
    "    \n",
    "    if isinput:\n",
    "        out = np.empty((n_samples*n_ops, 227, 227, 3), dtype=\"uint8\")        \n",
    "        for i in xrange(n_samples):\n",
    "            # original images resized to 227x227\n",
    "            out[i*n_ops:(i+1)*n_ops,:,:,:] = cv2.resize(mat[i,:,:,:],\n",
    "                                                 (227,227))\n",
    "            # 4 crops\n",
    "            out[i*n_ops+1:((i+1)*n_ops+1),:,:,:] = mat[i,:227,:227,:]\n",
    "            out[i*n_ops+2:((i+1)*n_ops+2),:,:,:] = mat[i,-227:,:227,:]\n",
    "            out[i*n_ops+3:((i+1)*n_ops+3),:,:,:] = mat[i,:227,-227:,:]\n",
    "            out[i*n_ops+4:((i+1)*n_ops+4),:,:,:] = mat[i,-227:,-227:,:]\n",
    "            \n",
    "            # 5 mirrors\n",
    "            for j in xrange(n_ops//2):\n",
    "                out[i*n_ops+5+j:((i+1)*n_ops+5+j),:,:,:] = \\\n",
    "                out[i*n_ops+5+j:((i+1)*n_ops+5+j),:,:,::-1]           \n",
    "        \n",
    "    else:\n",
    "        out = mat.repeat(n_ops, axis=0)\n",
    "                \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DenseLayer, Conv2DLayer, MaxPool2DLayer, DropoutLayer\n",
    "from lasagne.layers import SliceLayer, ConcatLayer\n",
    "import lasagne\n",
    "\n",
    "def build_model(input_var=None):\n",
    "    # Input layer\n",
    "    ''' \n",
    "    out: b x 227 x 227 x 3\n",
    "    '''\n",
    "    lin = InputLayer(\n",
    "        shape=(None, 227, 227, 3),\n",
    "        input_var=input_var\n",
    "        )\n",
    "\n",
    "    # ConvPool1\n",
    "    ''' \n",
    "    out: b x 96 x 27 x 27 \n",
    "    out.W: 96 x 3 x 11 x 11\n",
    "    '''\n",
    "    \"\"\" input was b01c, need to be bc01\"\"\"\n",
    "    l1 = Conv2DLayer(\n",
    "        lasagne.layers.dimshuffle(lin, (0,3,1,2)),\n",
    "        num_filters=96, filter_size=11, stride=4,\n",
    "        W = Ws['W_0'], b = bs['b_0'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "        )\n",
    "    l1 = MaxPool2DLayer(\n",
    "        l1, pool_size=3, stride=2\n",
    "        )\n",
    "\n",
    "    # ConvPool2: 2 groups\n",
    "    ''' \n",
    "    out: b x 256 x 13 x 13\n",
    "    out.W0/1: 128 x 48 x 5 x 5\n",
    "    '''\n",
    "    l1_0 = SliceLayer(l1, indices=slice(None,48), axis=1)\n",
    "    l2_0 = Conv2DLayer(\n",
    "        l1_0, num_filters=128, filter_size=5, stride=1, pad=2,\n",
    "        W = Ws['W0_1'], b = bs['b0_1'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "    l2_0p = MaxPool2DLayer(\n",
    "        l2_0, pool_size=3, stride=2\n",
    "    )\n",
    "\n",
    "    l1_1 = SliceLayer(l1, indices=slice(48, None), axis=1)\n",
    "    l2_1 = Conv2DLayer(\n",
    "        l1_1, num_filters=128, filter_size=5, stride=1, pad=2,\n",
    "        W = Ws['W1_1'], b = bs['b1_1'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "    l2_1p = MaxPool2DLayer(\n",
    "        l2_1, pool_size=3, stride=2\n",
    "    )\n",
    "\n",
    "    l2 = ConcatLayer([l2_0p,l2_1p], axis=1)\n",
    "\n",
    "    # Conv3\n",
    "    ''' \n",
    "    out: b x 384 x 13 x 13\n",
    "    out.W: 384 x 256 x 3 x 3\n",
    "    '''\n",
    "    l3 = Conv2DLayer(\n",
    "        l2, num_filters=384, filter_size=3, stride=1, pad='same',\n",
    "        W = Ws['W_2'], b = bs['b_2'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "\n",
    "    # Conv4: 2 groups\n",
    "    ''' \n",
    "    out: b x 384 x 13 x 13\n",
    "    out.W0/1: 192 x 192 x 3 x 3\n",
    "    '''\n",
    "    l3_0 = SliceLayer(l3, indices=slice(None,192), axis=1)\n",
    "    l4_0 = Conv2DLayer(\n",
    "        l3_0, num_filters=192, filter_size=3, stride=1, pad='same',\n",
    "        W = Ws['W0_3'], b = bs['b0_3'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "\n",
    "    l3_1 = SliceLayer(l3, indices=slice(192, None), axis=1)\n",
    "    l4_1 = Conv2DLayer(\n",
    "        l3_1, num_filters=192, filter_size=3, stride=1, pad='same',\n",
    "        W = Ws['W1_3'], b = bs['b1_3'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "\n",
    "    # ConvPool5: 2 groups\n",
    "    ''' \n",
    "    out: b x 256 x 6 x 6\n",
    "    out.W0/1: 128 x 192 x 3 x 3\n",
    "    '''\n",
    "    l5_0 = Conv2DLayer(\n",
    "        l4_0, num_filters=128, filter_size=3, stride=1, pad='same',\n",
    "        W = Ws['W0_4'], b = bs['b0_4'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "    l5_0p = MaxPool2DLayer(\n",
    "        l5_0, pool_size=3, stride=2\n",
    "    )\n",
    "\n",
    "    l5_1 = Conv2DLayer(\n",
    "        l4_1, num_filters=128, filter_size=3, stride=1, pad='same',\n",
    "        W = Ws['W1_4'], b = bs['b1_4'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "    l5_1p = MaxPool2DLayer(\n",
    "        l5_1, pool_size=3, stride=2\n",
    "    )\n",
    "\n",
    "    l5 = ConcatLayer([l5_0p,l5_1p], axis=1)\n",
    "\n",
    "    # FC6\n",
    "    ''' \n",
    "    out: b x 4096 (x 1 x 1)\n",
    "    out.W: 9216 x 4096\n",
    "    '''\n",
    "    l6 = DenseLayer(\n",
    "        lasagne.layers.dropout(l5, p=.5),\n",
    "        num_units=4096,\n",
    "        W = Ws['W_5'], b = bs['b_5'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "\n",
    "    # FC7\n",
    "    ''' \n",
    "    out: b x 4096 (x 1 x 1)\n",
    "    out.W: 4096 x 4096\n",
    "    '''\n",
    "    l7 = DenseLayer(\n",
    "        lasagne.layers.dropout(l6, p=.5),\n",
    "        num_units=4096,\n",
    "        W = Ws['W_6'], b = bs['b_6'],\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "    )\n",
    "\n",
    "    # FC8: replace last layer in AlexNet\n",
    "    ''' \n",
    "    out: b x 22\n",
    "    out.W: 4096 x 22\n",
    "    '''\n",
    "    l8 = DenseLayer(\n",
    "        l7, num_units=22,     \n",
    "        nonlinearity=lasagne.nonlinearities.softmax\n",
    "    )\n",
    "    return l8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net0 = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compiling Theano expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "\n",
    "\"\"\"Subtract IMAGE MEAN\"\"\"\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets), \"Input and target size mismatch\"\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model... \n",
      "compiling functions... \n"
     ]
    }
   ],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.imatrix('targets')\n",
    "\n",
    "print \"building model... \"\n",
    "net0 = build_model(input_var)\n",
    "\n",
    "print \"compiling functions... \"\n",
    "# Build loss function\n",
    "prediction = lasagne.layers.get_output(net0)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction,\n",
    "                                                   target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "# Create update expression for training\n",
    "# using RMSprop\n",
    "params = lasagne.layers.get_all_params(net0, \n",
    "                                       trainable=True)\n",
    "updates = lasagne.updates.rmsprop(loss, params, \n",
    "                                  learning_rate=1.0, rho=0.9, epsilon=1e-06)\n",
    "train_fn = theano.function([input_var, target_var], loss,\n",
    "                           updates=updates)\n",
    "\n",
    "## Building loss evaluation for validation set\n",
    "va_prediction = lasagne.layers.get_output(net0, \n",
    "                                          deterministic=True)\n",
    "va_loss = lasagne.objectives.categorical_crossentropy(va_prediction,\n",
    "                                                      target_var)\n",
    "va_loss = va_loss.mean()\n",
    "\n",
    "va_fn = theano.function([input_var, target_var], va_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = file('va_fn.pkl', 'wb')\n",
    "cPickle.dump(va_fn, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" remember to save after every X epoch(s)\"\"\"\n",
    "import time\n",
    "\n",
    "def main(num_epochs=500, batchsize=96, toy=True, \n",
    "         Xtr=Xtr, Ytr=Ytr, Xva=Xva, Yva=Yva):\n",
    "       \n",
    "    # Sanity check: try to overfit a tiny (20 instances) subset of the data\n",
    "    if toy: \n",
    "        np.random.RandomState(11)\n",
    "        idx = np.random.randint(0,Xtr.shape[0]/10,20) * 10\n",
    "        Xtr = Xtr[idx,:,:,:]\n",
    "        Ytr = Ytr[idx,:]\n",
    "        batchsize = 20\n",
    "\n",
    "    print \"Starting training with batchsize of %d ...\" %(batchsize)    \n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(Xtr, Ytr, batchsize, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        if not toy:\n",
    "            va_err = 0\n",
    "            va_batches = 0\n",
    "            for batch in iterate_minibatches(Xva, Yva, batchsize, shuffle=False):\n",
    "                inputs, targets = batch\n",
    "                err = va_fn(inputs, targets)\n",
    "                va_err += err\n",
    "                va_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        if not toy:\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(va_err / va_batches))\n",
    "\n",
    "            # Save the model \n",
    "            if epoch%2==0: \n",
    "                np.savez('Data/MSRCv2/model_'+str(epoch)+'.npz', lasagne.layers.get_all_param_values(net0))\n",
    "        \n",
    "        if toy and np.allclose(train_err,0):\n",
    "            print \"Error for toy problem is 0. Training finished\"\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with batchsize of 20 ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-0a65da2e6681>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(num_epochs, batchsize, toy, Xtr, Ytr, Xva, Yva)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoa/.conda/envs/multiLabEnv/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs=500\n",
    "batchsize=20\n",
    "toy=True\n",
    "\n",
    "# Sanity check: try to overfit a tiny (20 instances) subset of the data\n",
    "np.random.RandomState(11)\n",
    "idx = np.random.randint(0,Xtr.shape[0]/10,20) * 10\n",
    "Xtr_toy = Xtr[idx,:,:,:]\n",
    "Ytr_toy = Ytr[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr_toy.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "if np.any([isinstance(inputvar.op, T.Elemwise) for inputvar in train_fn.maker.fgraph.toposort()]):    print 'Used the cpu'\n",
    "else:\n",
    "    print 'Used the gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with batchsize of 20 ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cuda error 'an illegal instruction was encountered' while copying %lli data element to device memory\nApply node that caused the error: GpuFromHost(inputs)\nToposort index: 23\nInputs types: [TensorType(float32, 4D)]\nInputs shapes: [(20, 227, 227, 3)]\nInputs strides: [(618348, 2724, 12, 4)]\nInputs values: ['not shown']\nOutputs clients: [[GpuDimShuffle{0,3,1,2}(GpuFromHost.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-1f8587508742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mva_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr_toy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr_toy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoa/.conda/envs/multiLabEnv/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    619\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoa/.conda/envs/multiLabEnv/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    295\u001b[0m     exc_value = exc_type(str(exc_value) + detailed_err_msg +\n\u001b[0;32m    296\u001b[0m                          '\\n' + '\\n'.join(hints))\n\u001b[1;32m--> 297\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoa/.conda/envs/multiLabEnv/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cuda error 'an illegal instruction was encountered' while copying %lli data element to device memory\nApply node that caused the error: GpuFromHost(inputs)\nToposort index: 23\nInputs types: [TensorType(float32, 4D)]\nInputs shapes: [(20, 227, 227, 3)]\nInputs strides: [(618348, 2724, 12, 4)]\nInputs values: ['not shown']\nOutputs clients: [[GpuDimShuffle{0,3,1,2}(GpuFromHost.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "\n",
    "print \"Starting training with batchsize of %d ...\" %(batchsize)    \n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    train_err += va_fn(Xtr_toy, Ytr_toy)\n",
    "\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err))\n",
    "\n",
    "    if np.allclose(train_err,0):\n",
    "        print \"Error for toy problem is 0. Training finished\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
